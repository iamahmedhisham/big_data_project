
**Explanation**:
- **Database**: Creates `mimic_db` with a default HDFS location (`hdfs://namenode:9000/transformed_mimic`), though each table specifies its own location.
- **External Tables**: Tables are `EXTERNAL`, meaning Hive manages metadata but not the data. If a table is dropped, the Parquet files remain in HDFS.
- **Schema Mapping**:
  - Spark `integer` → Hive `INT`
  - Spark `string` → Hive `STRING` (no length limits, unlike `VARCHAR`)
  - Spark `timestamp` → Hive `TIMESTAMP`
  - Spark `double` → Hive `DOUBLE`
- **Location**: Each table points to its Parquet file (e.g., `hdfs://namenode:9000/transformed_mimic/ADMISSIONS.parquet`).
- **Parquet Format**: `STORED AS PARQUET` matches the file format, and `TBLPROPERTIES ('parquet.compression'='SNAPPY')` aligns with common Parquet compression (adjust if your files use a different compression, e.g., `GZIP`).
- **Nullable**: All columns are nullable, as per the schemas (`nullable = true`).

---

### Step 2: Set Up Hive in Your Docker Environment
Your current setup includes Spark and HDFS (via `namenode`), but Hive may not be installed. Below are steps to add Hive to your Dockerized environment.

#### Option 1: Add Hive to `spark-master` Container
Modify the `spark-master` container to include Hive.

1. **Create a Custom Dockerfile**:
   In `C:\Users\ICT012\Desktop\big_data_project`, create a `Dockerfile`:
   ```dockerfile
   FROM bitnami/spark:3.0.0
   # Install Hive
   RUN apt-get update && apt-get install -y wget
   RUN wget https://archive.apache.org/dist/hive/hive-2.3.7/apache-hive-2.3.7-bin.tar.gz && \
       tar -xzf apache-hive-2.3.7-bin.tar.gz -C /opt && \
       rm apache-hive-2.3.7-bin.tar.gz
   ENV HIVE_HOME=/opt/apache-hive-2.3.7-bin
   ENV PATH=$PATH:$HIVE_HOME/bin
   # Copy Hive configuration
   COPY hive-site.xml $HIVE_HOME/conf/hive-site.xml
   # Install Hadoop client for HDFS access
   RUN wget https://archive.apache.org/dist/hadoop/common/hadoop-3.2.1/hadoop-3.2.1.tar.gz && \
       tar -xzf hadoop-3.2.1.tar.gz -C /opt && \
       rm hadoop-3.2.1.tar.gz
   ENV HADOOP_HOME=/opt/hadoop-3.2.1
   ENV PATH=$PATH:$HADOOP_HOME/bin
   COPY core-site.xml $HADOOP_HOME/etc/hadoop/core-site.xml